# Invoke Singularity
#
universe   = vanilla
executable = train_hvt.sh
arguments = $(ClusterId) 4

should_transfer_files = YES
when_to_transfer_output = ON_EXIT_OR_EVICT

Log    = condor_log/$(Cluster).log
Output = condor_log/$(Cluster)-$(Process).out
Error  = condor_log/$(Cluster)-$(Process).err


# Log    = condor_log/$(Cluster).log
# Output = condor_out/$(Cluster)-$(Process).out
# Error  = condor_err/$(Cluster)-$(Process).err

# Enable Singularity feature
## Notre Dame Images on CVMFS
# +SingularityImage = "/cvmfs/singularity.opensciencegrid.org/notredamedulac/el7-pytorch-gpu:latest"
# /afs/crc.nd.edu/x86_64_linux/s/singularity/images/amber18.sif

# PyTorch Image on AFS
+SingularityImage = "/afs/crc.nd.edu/x86_64_linux/s/singularity/images/pytorch-1.9.sif"

# requirements = regexp("V100 | RTX 6000 | A10", TARGET.CUDADeviceName)

# request_cpus   = 12
# Requirements = (regexp("V100", TARGET.CUDADeviceName) && machine == "qa-v100-011.crc.nd.edu")
# requirements = regexp("V100 | RTX 6000 | GTX 1080 Ti | TITAN Xp", CUDADeviceName)
# Requirements = (CUDADeviceName == "Tesla V100-PCIE-32GB")
# requirements = regexp("RTX 6000", TARGET.CUDADeviceName)

# requirements = machine == "ta-a6k-004.crc.nd.edu"

# requirements = machine == "qa-a10-001.crc.nd.edu"

requirements = machine == "sa-rtx6ka-002.crc.nd.edu"

# requirements = machine == "qa-a100-005.crc.nd.edu"

# requirements = machine == "qa-v100-005.crc.nd.edu"

# requirements = machine == "qa-v100-005.crc.nd.edu"

# request_gpus = 1

request_memory = 40 Gb

# request_memory = 64 Gb
request_cpus   = 12
request_disk = 30 Gb

Queue 1
